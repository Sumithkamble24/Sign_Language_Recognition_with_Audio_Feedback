# Sign_Language_Recognition_with_Audio_Feedback
The core deliverable of this project is a real-time Indian Sign Language (ISL) recognition system powered by a Convolutional Neural Network (CNN). The system is trained on a labeled dataset of grayscale images representing the ISL alphabet, with each image resized to 64x64 pixels and normalized for consistency. Class labels were encoded using one-hot encoding. The model architecture consists of multiple layers, including convolutional and max-pooling layers for feature extraction, followed by flattening and dense layers for classification, and ends with a softmax output layer. The model was compiled with the Adam optimizer and categorical cross-entropy loss function and trained over several epochs. It achieved high accuracy of 99% and acceptable loss level of 1.4, which were validated using a train-test split. After training, the model was saved in the form of sign_language_model.h5, along with a classes.txt file containing all recognized labels for future use.
<img width="16326" height="88" alt="image" src="https://github.com/user-attachments/assets/f2ee35b2-a36a-4375-b121-69ef71b6dfbe" />
